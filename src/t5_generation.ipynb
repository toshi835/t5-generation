{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5-generation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H2pYAj2HqSUD",
        "5Zu3gSBHp7hw",
        "hpAbcU9mqD61"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5UDvWycqdTu"
      },
      "source": [
        "# 任意のデータを使ったT5での文章生成\n",
        "\n",
        "使い方\n",
        "*   ランタイムから「ランタイムのタイプの変更」を行い、GPUを選択する\n",
        "*   左のファイルマークを選択し、csvデータをアップロードする  \n",
        "*   ランタイムから「すべてのセルを実行」を選択\n",
        "*   結果を確認する\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2pYAj2HqSUD"
      },
      "source": [
        "## ライブラリのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXCuItv8lFTP"
      },
      "source": [
        "!pip install -qU torch==1.7.1 torchtext==0.8.0 torchvision==0.8.2\n",
        "!pip install -q transformers==4.4.2 pytorch_lightning==1.2.1 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bspHwN3skzi6"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ_CuJyhlYli"
      },
      "source": [
        "# 乱数シードの設定\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zu3gSBHp7hw"
      },
      "source": [
        "## 前処理(preprocess.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnxrgFVSkl9q"
      },
      "source": [
        "df = pd.read_csv(\"data.csv\", index_col=0)\n",
        "df_train, df_devtest = train_test_split(df, random_state=42)\n",
        "df_dev, df_test = train_test_split(df_devtest, test_size=0.5, random_state=42)\n",
        "\n",
        "df_train.to_csv(\"./train.tsv\", sep=\"\\t\")\n",
        "df_dev.to_csv(\"./dev.tsv\", sep=\"\\t\")\n",
        "df_test.to_csv(\"./test.tsv\", sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpAbcU9mqD61"
      },
      "source": [
        "## 学習(train.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j13WWUadlZQd"
      },
      "source": [
        "class TsvDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512):\n",
        "        self.file_path = os.path.join(data_dir, type_path)\n",
        "\n",
        "        self.input_max_len = input_max_len\n",
        "        self.target_max_len = target_max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        source_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": source_mask,\n",
        "                \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "    def _make_record(self, input, target):\n",
        "        # ニュースタイトル生成タスク用の入出力形式に変換する。\n",
        "        input = f\"{input}\"\n",
        "        target = f\"{target}\"\n",
        "        return input, target\n",
        "\n",
        "    def _build(self):\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.strip().split(\"\\t\")\n",
        "                if i == 0:  # header\n",
        "                    continue\n",
        "                input = line[1]\n",
        "                target = line[0]\n",
        "\n",
        "                input, target = self._make_record(input, target)\n",
        "\n",
        "                tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                    [input], max_length=self.input_max_len, truncation=True,\n",
        "                    padding=\"max_length\", return_tensors=\"pt\"\n",
        "                )\n",
        "                # tokenizer.batch_encode_plus([input], max_length=100, truncation=True,padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "                tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                    [target], max_length=self.target_max_len, truncation=True,\n",
        "                    padding=\"max_length\", return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                self.inputs.append(tokenized_inputs)\n",
        "                self.targets.append(tokenized_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBH5Ef9qleZe"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # 事前学習済みモデルの読み込み\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "\n",
        "        # トークナイザーの読み込み\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path, is_fast=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None,\n",
        "                decoder_attention_mask=None, labels=None):\n",
        "        \"\"\"順伝搬\"\"\"\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        \"\"\"ロス計算\"\"\"\n",
        "        labels = batch[\"target_ids\"]\n",
        "\n",
        "        # All labels set to -100 are ignored (masked),\n",
        "        # the loss is only computed for labels in [0, ..., config.vocab_size]\n",
        "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"訓練ステップ処理\"\"\"\n",
        "        loss = self._step(batch)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"バリデーションステップ処理\"\"\"\n",
        "        loss = self._step(batch)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return {\"val_loss\": loss}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"テストステップ処理\"\"\"\n",
        "        loss = self._step(batch)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return {\"test_loss\": loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"オプティマイザーとスケジューラーを作成する\"\"\"\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters()\n",
        "                           if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters()\n",
        "                           if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                          lr=self.hparams.learning_rate,\n",
        "                          eps=self.hparams.adam_epsilon)\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.hparams.warmup_steps,\n",
        "            num_training_steps=self.t_total\n",
        "        )\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n",
        "\n",
        "    def get_dataset(self, tokenizer, type_path, args):\n",
        "        \"\"\"データセットを作成する\"\"\"\n",
        "        return TsvDataset(\n",
        "            tokenizer=tokenizer,\n",
        "            data_dir=args.data_dir,\n",
        "            type_path=type_path,\n",
        "            input_max_len=args.max_input_length,\n",
        "            target_max_len=args.max_target_length)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"初期設定（データセットの読み込み）\"\"\"\n",
        "        if stage == 'fit' or stage is None:\n",
        "            train_dataset = self.get_dataset(tokenizer=self.tokenizer,\n",
        "                                             type_path=\"train.tsv\", args=self.hparams)\n",
        "            self.train_dataset = train_dataset\n",
        "\n",
        "            val_dataset = self.get_dataset(tokenizer=self.tokenizer,\n",
        "                                           type_path=\"dev.tsv\", args=self.hparams)\n",
        "            self.val_dataset = val_dataset\n",
        "\n",
        "            self.t_total = (\n",
        "                    (len(train_dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "                    // self.hparams.gradient_accumulation_steps\n",
        "                    * float(self.hparams.num_train_epochs)\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"訓練データローダーを作成する\"\"\"\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          batch_size=self.hparams.train_batch_size,\n",
        "                          drop_last=True, shuffle=True, num_workers=4)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"バリデーションデータローダーを作成する\"\"\"\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          batch_size=self.hparams.eval_batch_size,\n",
        "                          num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwWlbAkOk5RJ"
      },
      "source": [
        "# 事前学習済みモデル\n",
        "PRETRAINED_MODEL_NAME = \"sonoisa/t5-base-japanese\"\n",
        "\n",
        "# モデルとデータの保存先\n",
        "DATA_DIR=\"./\"\n",
        "MODEL_DIR=\"./\"\n",
        "\n",
        "# GPU利用有無\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "# 各種ハイパーパラメータ\n",
        "args_dict = dict(\n",
        "    data_dir=DATA_DIR,  # データセットのディレクトリ\n",
        "    model_dir=MODEL_DIR,  # データセットのディレクトリ\n",
        "    model_name_or_path=PRETRAINED_MODEL_NAME,\n",
        "    tokenizer_name_or_path=PRETRAINED_MODEL_NAME,\n",
        "\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    n_gpu=1 if USE_GPU else 0,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False,\n",
        "    opt_level='O1',\n",
        "    max_grad_norm=1.0,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# トークナイザー（SentencePiece）モデルの読み込み\n",
        "tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL_NAME, is_fast=True)\n",
        "\n",
        "# テストデータセットの読み込み\n",
        "train_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], \"train.tsv\",\n",
        "                           input_max_len=512, target_max_len=64)\n",
        "\n",
        "# 学習に用いるハイパーパラメータを設定する\n",
        "args_dict.update({\n",
        "    \"max_input_length\": 512,  # 入力文の最大トークン数\n",
        "    \"max_target_length\": 64,  # 出力文の最大トークン数\n",
        "    \"train_batch_size\": 4,  # 訓練時のバッチサイズ\n",
        "    \"eval_batch_size\": 8,  # テスト時のバッチサイズ\n",
        "    \"num_train_epochs\": 9,  # 訓練するエポック数\n",
        "})\n",
        "args = argparse.Namespace(**args_dict)\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    precision=16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        ")\n",
        "\n",
        "# 転移学習の実行\n",
        "model = T5FineTuner(args)\n",
        "trainer = pl.Trainer(**train_params)\n",
        "trainer.fit(model)\n",
        "\n",
        "# 最終エポックのモデルを保存\n",
        "model.tokenizer.save_pretrained(args.model_dir)\n",
        "model.model.save_pretrained(args.model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBH6bFN4qJU3"
      },
      "source": [
        "## テスト(test.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAlmcDLSnEaI"
      },
      "source": [
        "# トークナイザー（SentencePiece）\n",
        "tokenizer = T5Tokenizer.from_pretrained(args.model_dir, is_fast=True)\n",
        "\n",
        "# 学習済みモデル\n",
        "trained_model = T5ForConditionalGeneration.from_pretrained(args.model_dir)\n",
        "\n",
        "# GPUの利用有無\n",
        "if USE_GPU:\n",
        "    trained_model.cuda()\n",
        "\n",
        "# テストデータの読み込み\n",
        "test_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], \"test.tsv\",\n",
        "                          input_max_len=args.max_input_length,\n",
        "                          target_max_len=args.max_target_length)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=args.eval_batch_size, num_workers=4)\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "targets = []\n",
        "\n",
        "BEAM_SIZE = 1\n",
        "# beamが複数のときのパラメータ\n",
        "beam_params = dict()\n",
        "if BEAM_SIZE != 1:\n",
        "    cnt = 0\n",
        "    beam_params.update({\n",
        "        \"num_beams\": BEAM_SIZE,  # ビームサーチの探索幅\n",
        "        \"diversity_penalty\": 1.0,  # 生成結果の多様性を生み出すためのペナルティ\n",
        "        \"num_beam_groups\": BEAM_SIZE,  # ビームサーチのグループ数\n",
        "        \"num_return_sequences\": BEAM_SIZE,  # 生成する文の数\n",
        "    })\n",
        "\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids = batch['source_ids']\n",
        "    input_mask = batch['source_mask']\n",
        "\n",
        "    if USE_GPU:\n",
        "        input_ids = input_ids.cuda()\n",
        "        input_mask = input_mask.cuda()\n",
        "\n",
        "    output = trained_model.generate(input_ids=input_ids,\n",
        "                                    attention_mask=input_mask,\n",
        "                                    max_length=args.max_target_length,\n",
        "                                    temperature=1.0,  # 生成にランダム性を入れる温度パラメータ\n",
        "                                    repetition_penalty=1.5,  # 同じ文の繰り返し（モード崩壊）へのペナルティ\n",
        "                                    **beam_params\n",
        "                                    )\n",
        "\n",
        "    output_text = [tokenizer.decode(ids, skip_special_tokens=True,\n",
        "                                    clean_up_tokenization_spaces=False)\n",
        "                   for ids in output]\n",
        "    target_text = [tokenizer.decode(ids, skip_special_tokens=True,\n",
        "                                    clean_up_tokenization_spaces=False)\n",
        "                   for ids in batch[\"target_ids\"]]\n",
        "    input_text = [tokenizer.decode(ids, skip_special_tokens=True,\n",
        "                                   clean_up_tokenization_spaces=False)\n",
        "                  for ids in input_ids]\n",
        "\n",
        "    inputs.extend(input_text)\n",
        "    outputs.extend(output_text)\n",
        "    targets.extend(target_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IANODmcWoY0g"
      },
      "source": [
        "with open(os.path.join(args.data_dir, \"output.txt\"), \"w\") as out, open(os.path.join(args.data_dir, \"target.txt\"),\n",
        "                                                                      \"w\") as tar, open(\n",
        "    os.path.join(args.data_dir, \"input.txt\"), \"w\") as inp:\n",
        "    for i in range(len(inputs)):\n",
        "        print(\"generated: \" + \"\\n\\t\".join(outputs[i * BEAM_SIZE:i * BEAM_SIZE + BEAM_SIZE]))\n",
        "        print(\"target:    \" + targets[i])\n",
        "        print(\"src:       \" + inputs[i])\n",
        "        print()\n",
        "        out.write(\", \".join(outputs[i * BEAM_SIZE:i * BEAM_SIZE + BEAM_SIZE]) + \"\\n\")\n",
        "        tar.write(targets[i] + \"\\n\")\n",
        "        inp.write(inputs[i] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}